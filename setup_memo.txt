Intel RealSenseで指の角度を測る

Author：上堀内優輝
Date：
はじめに
　ここでは、Intel RealSenseを用いて指の角度を測るために必要なパッケージなどのインストール方法や、プログラムを書く際に参考にしたサイトなどをまとめた。これを読めば、１からセットアップして指の角度を測れるようになる。
　使用した環境やOSは以下の通りである。
	OS：Ubuntu22.04
	ROS：ROS ２ Humble
	RealSense：D435
　上記のように、OSにはUbuntuを用いたので、PCのデフォルトのOSがUbuntuでない場合は、デュアルブートを行う必要があるが、ここでは割愛する。Windows11とUbuntu22.04のデュアルブートの方法は、次のサイトを参考にするとよい。
https://diary-039.com/entry/2022/05/15/dual-boot_windows11-ubuntu2204

1.	ROS2 Humbleのインストール
まず、Ubuntu22.04にROS2Humbleをインストールする。ここでは、次のサイトを参考にした。
https://qiita.com/porizou1/items/5dd915402e2990e4d95f
https://docs.ros.org/en/humble/Installation/Ubuntu-Install-Debians.html
　Ubuntuでターミナルを開き、次のコマンドを実行する。
・aptリポジトリの追加
sudo apt update && sudo apt install curl gnupg lsb-release
sudo curl -sSL https://raw.githubusercontent.com/ros/rosdistro/master/ros.key -o /usr/share/keyrings/ros-archive-keyring.gpg

・リポジトリをsource listに追加
 echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/ros-archive-keyring.gpg] http://packages.ros.org/ros2/ubuntu $(source /etc/os-release && echo $UBUNTU_CODENAME) main" | sudo tee /etc/apt/sources.list.d/ros2.list > /dev/null

・ROS2 インストール
 sudo apt update
sudo apt install ros-humble-desktop

※動作テスト
Talkerを実行
 source /opt/ros/humble/setup.bash
ros2 run demo_nodes_cpp talker

別のターミナルを開いて、Listenerを実行
 source /opt/ros/humble/setup.bash
ros2 run demo_nodes_py listener

・環境設定
 echo "source /opt/ros/humble/setup.bash" >> ~/.bashrc
source .bashrc

・colconのインストール
 sudo apt install python3-colcon-common-extensions

・Gazeboのインストール
 sudo apt -y install gazebo
sudo apt install ros-humble-gazebo-*

・rqtのプラグインをインストール
 sudo apt install ros-humble-rqt-*

 
2.	RealSenseを使用できるようにする手順
次に、Ubuntu22.04、ROS2Humble環境でRealSenseを使用できるようにする。ここでは、次のサイトを参考にした。
https://qiita.com/porizou1/items/9d44053dbce648f1470d
https://rt-net.jp/humanoid/archives/4815
引き続きUbuntuのターミナルで以下のコマンドを実行する。

Intel RealSense SDK のインストール
・公開鍵を登録
 sudo mkdir -p /etc/apt/keyrings
curl -sSf https://librealsense.intel.com/Debian/librealsense.pgp | sudo tee /etc/apt/keyrings/librealsense.pgp > /dev/null

・サーバーをレポジトリリストに登録
 echo "deb [signed-by=/etc/apt/keyrings/librealsense.pgp] https://librealsense.intel.com/Debian/apt-repo `lsb_release -cs` main" | \
sudo tee /etc/apt/sources.list.d/librealsense.list
sudo apt-get update

・ライブラリをインストール
 sudo apt-get install librealsense2-dkms librealsense2-utils librealsense2-dev librealsense2-dbg

※動作確認
RealSenseをPCのUSB3.0に接続し、以下コマンドでrealsense-viewerを起動する。
 realsense-viewer

realsense-rosをインストール
・ワークスペースの作成
 mkdir -p ~/hand_sense_ws/src
cd ~/hand_sense_ws/src/

・realsense-rosをインストールしてcolcon build
 git clone https://github.com/IntelRealSense/realsense-ros.git
cd ~/hand_sense_ws
colcon build --symlink-install

・デフォルトのbranchはros2-developmentになっているのでこれを使用（私はここで何かエラーが出たが結果的にうまくいっていたのでよしとする）。
 cd src/realsense-ros/
git branch
* ros2-development

3.	pip3の導入とモジュールのインストール
　参考にしたサイトの多くがpythonでプログラムを書いていたので、ここでもpythonを用いる。Ubuntuは最初からpythonがインストールされており、そのまま使用できるが、pythonで使うモジュールのインストールにはpip3を使うので、これを導入する。ここでは以下のサイトを参考にした。
https://qiita.com/HiroRittsu/items/e58063fb74d799d37cc4
　引き続きUbuntuのターミナルから以下のコマンドを実行する。
・pip3の導入
 curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py
sudo python3 get-pip.py

・pip3自体のアップデート
 pip3 install -U pip

・モジュールのインストール
 pip3 install pyrealsense2
 pip3 install mediapipe
 pip3 install opencv-python
pip3 install opencv-contrib-python

4.	PubSub通信を行うためのパッケージの作成
　RealSenseから送られてきたデータを受け取るために、PubSub通信を行うパッケージを作る。ここでは、以下の動画を参考にした。
https://www.youtube.com/watch?v=pTM2yIPEHo0
　引き続きUbuntuのターミナルから以下のコマンドを実行する。
・ワークスペースに移動
 cd ~/hand_sense_ws/src/

・パッケージの作成
 ros2 pkg create subscriber –build-type ament_python
 cd subscriber

・上記のフォルダ内にsetup.pyというファイルがあるのでそれを開き、以下のように書きかえる。
22        'console_scripts': [
23        ],
↓
22        'console_scripts': [
23            "subscriber = subscriber.subscriber:main"
24        ],

5.	RealSenseからのデータの受信
RealSenseからデータを受け取る手順を示す。ここでは、以下のサイトを参考にした。
https://ar-ray.hatenablog.com/entry/2023/11/30/083000
https://github.com/IntelRealSense/realsense-ros/tree/ros2-development#rgbd-topic
　引き続きUbuntuのターミナルから以下のコマンドを実行する。
・ワークスペースに移動し、ソースコードからビルド
 cd ~/hand_sense_ws
 git clone https://github.com/IntelRealSense/realsense-ros.git src/realsense-ros
 source /opt/ros/humble/setup.bash
 colcon build

・カメラの起動
source ~/hand_sense_ws/install/setup.bash
ros2 launch realsense2_camera rs_launch.py enable_rgbd:=true enable_sync:=true align_depth.enable:=true enable_color:=true enable_depth:=true

※動作確認
別のターミナルを開いて、以下のコマンドを実行した時に、/camera/camera/rgbdが出力されていることを確認する。
 Ros2 topic list

ディレクトリを移動し、pythonファイルを作成
 cd ~/hand_sense_ws/src/subscriber/subscriber
 touch rgbd_test.py

次のようにpythonプログラムを書く
rgbd_test.py
import cv2
from cv_bridge import CvBridge
import numpy as np
import rclpy
from rclpy.node import Node
from realsense2_camera_msgs.msg import RGBD

min_depth, max_depth = 200, 500 # mm

class RsSub(Node):
    def __init__(self):
        super().__init__('minimal_subscriber')
        self.bridge = CvBridge()
        self.subscription = self.create_subscription(RGBD, '/camera/camera/rgbd', self.listener_callback, 10)

    def listener_callback(self, msg):
        cv2.imshow("Image window", self.mask_rgb(self.bridge.imgmsg_to_cv2(msg.rgb, "bgr8"), self.bridge.imgmsg_to_cv2(msg.depth, "passthrough")))
        cv2.waitKey(1)

    def mask_rgb(self, rgb, depth) -> np.ndarray:
        mask = (depth <= min_depth) | (depth >= max_depth)
        return np.where(np.broadcast_to(mask[:, :, None], rgb.shape), 0, rgb).astype(np.uint8)

rclpy.init(args=None)
rclpy.spin(RsSub())

上記ファイルを実行
 source ~/hand_sense_ws/install/setup.bash
python3 rgbd_test.py

これにより、カメラからの距離が20cm以上50cm以下の部分以外が黒く塗りつぶされた画像が出力されていればよい。

6.	指の角度を測る
指の角度を測るプログラムをします。なお、手の骨格を検出するために、MediaPipeを用いた。MediaPipeについては、以下のサイトを参考にした。
https://elsammit-beginnerblg.hatenablog.com/entry/2022/01/13/225928
https://developers.google.com/mediapipe/solutions/vision/hand_landmarker
　Ubuntuのターミナルを開き、RealSenseを起動する。
source ~/hand_sense_ws/install/setup.bash
ros2 launch realsense2_camera rs_launch.py enable_rgbd:=true enable_sync:=true align_depth.enable:=true enable_color:=true enable_depth:=true

　別のターミナルを開き、ディレクトリを移動し、pythonファイルを作成する。
 cd ~/hand_sense_ws/src/subscriber/subscriber
 touch angle_finger.py



次のようにpythonプログラムを書く
angle_finger.py
import cv2
from cv_bridge import CvBridge
import numpy as np
#import rospy
import rclpy
from rclpy.node import Node
import pyrealsense2 as rs
from realsense2_camera_msgs.msg import RGBD
import mediapipe as mp
import csv
#import pprint
import time
import matplotlib.pyplot as plt
#from matplotlib import animation
import fcntl
import termios
import sys
import os
from datetime import datetime
 
now = datetime.now()
time_start = now.strftime("%Y%m%d%H%M%S")
new_dir_path = "data/"+time_start
os.mkdir(new_dir_path)
f = open(new_dir_path+"/data_"+time_start+".csv", "w", newline="")
writer = csv.writer(f)
title_list = []
finger_list = ['time_count', 'THUMB_', 'INDEX_', 'MIDDLE_', 'RING_', 'PINKY_']
title_list.append(finger_list[0])
count = 1
finger_count = 1
for i in range(15):
    string = finger_list[finger_count] + str(4 - count)
    title_list.append(string)
    if count == 3:
        count = 0
        finger_count += 1
    count += 1
writer.writerow(title_list)
fps = 25
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
video = cv2.VideoWriter(new_dir_path+"/video_"+time_start+".mp4", fourcc, fps, (640, 480))
start_time = time.perf_counter()
 
 
class RsSub(Node):
    mp_drawing = mp.solutions.drawing_utils
    mp_drawing_styles = mp.solutions.drawing_styles
    mp_hands = mp.solutions.hands
    StartFlg = False
    rsp = ""
    FinishFlg = False
    hands = mp_hands.Hands(
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5)
    def __init__(self):
        super().__init__('minimal_subscriber')
        self.bridge = CvBridge()
        self.subscription = self.create_subscription(RGBD, '/camera/camera/rgbd', self.listener_callback, 10)
 
    def listener_callback(self, msg):
        num_node = 21
        num_edge = 20
        num_joint = 15
        array_rgb = self.bridge.imgmsg_to_cv2(msg.rgb, "bgr8")
        array_depth = self.bridge.imgmsg_to_cv2(msg.depth, "passthrough")
        image, index_finger = self.hand_skelton(array_rgb, num_node)
        #cv2.imshow("Image window", array_rgb)
        cameraInfo = msg.depth_camera_info
        intrinsics = rs.intrinsics()
        intrinsics.width = cameraInfo.width
        intrinsics.height = cameraInfo.height
        intrinsics.ppx = cameraInfo.k[2]
        intrinsics.ppy = cameraInfo.k[5]
        intrinsics.fx = cameraInfo.k[0]
        intrinsics.fy = cameraInfo.k[4]
        #intrinsics.model = cameraInfo.distortion_model
        intrinsics.model  = rs.distortion.none     
        intrinsics.coeffs = [i for i in cameraInfo.d]
        node_point = np.zeros((num_node, 3))
        for i in range(num_node):
            node_point[i, :] = rs.rs2_deproject_pixel_to_point(intrinsics, index_finger[i, :], array_depth[index_finger[i, 1], index_finger[i, 0]])
        #print(node_point)
        edge = np.zeros((num_edge, 3))
        for i in range(num_edge):
            r = i % 4
            if r == 0:
                j_0 = 0
            else:
                j_0 = i
            j_1 = i + 1
            edge_0 = []
            for a, b in zip(node_point[j_1, :], node_point[j_0, :]):
                edge_0.append(a - b)
            edge[i, :] = edge_0
        angle_joint = np.zeros(num_joint + 1)
        angle_joint[0] = time.perf_counter() - start_time
        count_joint = 0
        for i in range(num_joint):
            r = i % 3
            if r == 0:
                count_joint += 1
            j_0 = count_joint - 1
            j_1 = count_joint
            v_finger_0 = -edge[j_0, :]
            v_finger_1 = edge[j_1, :]
            v_dot = np.dot(v_finger_0, v_finger_1)
            v_abs_0 = np.sqrt(np.dot(v_finger_0, v_finger_0))
            v_abs_1 = np.sqrt(np.dot(v_finger_1, v_finger_1))
            v_abs = v_abs_0 * v_abs_1
            COS = v_dot / v_abs
            SITA = np.arccos(COS)
            angle_joint[i + 1] = 180 - SITA * 180 / np.pi
            count_joint += 1
        print("angle_finger", angle_joint)
        writer.writerow(angle_joint)
        cv2.imshow("Image window", image)
        #ims.append(image)
        video.write(image)
        cv2.waitKey(1) 
    
    def hand_skelton(self, image, num_node):
        image_width, image_height = image.shape[1], image.shape[0]
        finger = np.zeros((num_node, 2), dtype=np.int64)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        #print(image.shape)
        results = self.hands.process(image)
        if results.multi_hand_landmarks:
            for hand_landmarks in results.multi_hand_landmarks:
                self.mp_drawing.draw_landmarks(
                    image,
                    hand_landmarks,
                    self.mp_hands.HAND_CONNECTIONS,
                    self.mp_drawing_styles.get_default_hand_landmarks_style(),
                    self.mp_drawing_styles.get_default_hand_connections_style())
                for i in range(num_node):
                    finger[i, 0] = int(hand_landmarks.landmark[i].x * image_width)
                    finger[i, 1] = int(hand_landmarks.landmark[i].y * image_height)
                finger[:, 0] = self.cut_pixel(finger[:, 0], image_width)
                finger[:, 1] = self.cut_pixel(finger[:, 1], image_height)
        
        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
        return image, finger
    
    def cut_pixel(self, pixel, max_pixel):
        pixel = np.where(pixel < 0, 0, pixel)
        pixel = np.where(pixel >= max_pixel, max_pixel - 1, pixel)
        return pixel
 
def main(args = None):
    rclpy.init(args = args)
    intel_subscriber = RsSub()
    #rclpy.spin(intel_subscriber)
    while True:
        rclpy.spin_once(intel_subscriber)
        key = getkey()
        # enterで終了
        if key == 10:
            break
    #anim = animation.ArtistAnimation(cv2, ims)
    #anim.save("angle_finger.mp4", writer="ffmreg")
    f.close()
    video.release()
    fig = plt.figure(figsize=[20, 5])
    f_0 = open(new_dir_path+"/data_"+time_start+".csv")
    reader = csv.reader(f_0)
    data = [row for row in reader]
    data = np.array(data)
    data = data.T
    length = data.shape[1]
    color_list = ["black", "dimgray", "darkgray", "brown", "red", "lightsalmon", "goldenrod", "yellow", "orange", "darkgreen", "limegreen", "yellowgreen", "purple", "blue", "aqua"]
    for i in range(15):
        plt.plot([float(v) for v in data[0, 1:length]], [float(v_0) for v_0 in data[i+1, 1:length]], color = color_list[i], label = data[i+1, 0], linewidth = 1)
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0)
    plt.xlabel("time[s]")
    plt.ylabel("angle[°]")
    plt.title("angle of finger")
    fig.savefig(new_dir_path+"/angle_finger_"+time_start+".png")
    plt.close(fig)
    for i in range(15):
        fig = plt.figure(figsize=[20, 5])
        plt.plot([float(v) for v in data[0, 1:length]], [float(v_0) for v_0 in data[i+1, 1:length]], color = color_list[i], label = data[i+1, 0], linewidth = 1)
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0)
        plt.xlabel("time[s]")
        plt.ylabel("angle[°]")
        plt.title("angle of "+data[i+1, 0])
        fig.savefig(new_dir_path+"/angle_"+data[i+1, 0]+"_"+time_start+".png")
        plt.close(fig)
    f_0.close()
    intel_subscriber.destroy_node()
    rclpy.shutdown()
 
def getkey():
    fno = sys.stdin.fileno()
 
    #stdinの端末属性を取得
    attr_old = termios.tcgetattr(fno)
 
    # stdinのエコー無効、カノニカルモード無効
    attr = termios.tcgetattr(fno)
    attr[3] = attr[3] & ~termios.ECHO & ~termios.ICANON # & ~termios.ISIG
    termios.tcsetattr(fno, termios.TCSADRAIN, attr)
 
    # stdinをNONBLOCKに設定
    fcntl_old = fcntl.fcntl(fno, fcntl.F_GETFL)
    fcntl.fcntl(fno, fcntl.F_SETFL, fcntl_old | os.O_NONBLOCK)
 
    chr = 0
 
    try:
        # キーを取得
        c = sys.stdin.read(1)
        if len(c):
            while len(c):
                chr = (chr << 8) + ord(c)
                c = sys.stdin.read(1)
    finally:
        # stdinを元に戻す
        fcntl.fcntl(fno, fcntl.F_SETFL, fcntl_old)
        termios.tcsetattr(fno, termios.TCSANOW, attr_old)
 
    return chr
 
 
if __name__ == "__main__":
    main()

上記ファイルを実行
 source ~/hand_sense_ws/install/setup.bash
python3 angle_finger.py


注記
　私自身、見よう見まねでセットアップを行ったので、上記の手順が完璧なものとは言えないかもしれないが、ご容赦いただきたい。
　また、手順に抜けがあるかもしれないので、躓いたら、エラーメッセージに従ってほしい。
